{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_file = './results_pipeline_tr_filtered/RDRS_train_filtered_4_NER_2391028.out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#если несколько фолдов\n",
    "from os.path import dirname, realpath, splitext, basename\n",
    "\n",
    "folds_sep = '(?<=\\n)\\d+\\n'\n",
    "with open(inp_file, 'r') as f:\n",
    "    txt = f.read()\n",
    "    \n",
    "seps = '\\n\\d\\n|^\\d+\\n'\n",
    "fold_nums = re.findall(seps, txt)\n",
    "fold_nums = list(map(lambda x: re.sub('\\D', '', x), fold_nums))\n",
    "\n",
    "\n",
    "txt_parts = re.split(folds_sep, txt)\n",
    "txt_parts = [txt_part for txt_part in txt_parts if txt_part]\n",
    "if len(txt_parts)!=len(fold_nums):\n",
    "    if txt_parts[0].find('Evaluation')<0:\n",
    "        txt_parts = txt_parts[1:]\n",
    "    elif txt_parts[-1].find('Evaluation')<0:\n",
    "        txt_parts = txt_parts[:-1]\n",
    "    else:\n",
    "        raise AssertionError\n",
    "    assert len(txt_parts)==len(fold_nums)\n",
    "\n",
    "filename = splitext(basename(inp_file))[0]\n",
    "for i, fold in enumerate(fold_nums):\n",
    "    fold_filename = fold + '_' + filename + '.out'\n",
    "    with open(dirname(realpath(inp_file)) + '/' + fold_filename, 'w') as f:\n",
    "        f.write(txt_parts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#если несколько фолдов\n",
    "import os\n",
    "\n",
    "for file in os.listdir(dirname(realpath(inp_file))):\n",
    "    if not file[0].isdigit() or file.find('csv')>0:\n",
    "        continue\n",
    "    fold_num = file.split('_')[0]\n",
    "    #читаем файл, делим на сперт репорты\n",
    "    with open(dirname(realpath(inp_file)) + '/' + file, 'r') as f:\n",
    "        txt = f.read()\n",
    "    txt_parts = re.split('Evaluation\\n\\n|--------------------------------------------------', txt)\n",
    "    txt_parts = [txt_part for txt_part in txt_parts if txt_part]\n",
    "    spert_reports = [txt_part for txt_part in txt_parts if re.search('--- Entities .+\\s+---', txt_part)]\n",
    "\n",
    "    spert_reports = [re.sub('\\n\\s+', '\\n', rep) for rep in spert_reports]\n",
    "\n",
    "\n",
    "    #основной код\n",
    "    def count_symbols_before_first_whitespace(matchobj):\n",
    "        num_symbols = matchobj.span()[0]\n",
    "        symbols_before_first_whitespace.append(num_symbols)\n",
    "        return '<'+str(num_symbols)+'>'\n",
    "\n",
    "    def make_valid_margin(matchobj):\n",
    "        margin = max_margin - int(matchobj.group(0)[1:-1])\n",
    "        return ' '*margin\n",
    "\n",
    "    def delete_n_first_whitespaces(matchobj):\n",
    "        columns = matchobj.group()\n",
    "        first_n_whitespaces = re.match('\\s+', columns).span()[1]\n",
    "        return ' '*(first_n_whitespaces-delete_n_first_whitespaces.n)+columns[first_n_whitespaces:]\n",
    "\n",
    "\n",
    "\n",
    "    for rep_id in range(len(spert_reports)):\n",
    "        sents = re.split('\\n+', spert_reports[rep_id])\n",
    "        #sents = spert_reports[rep_id].split('\\n')\n",
    "        symbols_before_first_whitespace = []\n",
    "        for sent_id in range(1, len(sents)):\n",
    "            #считаем кол-во символов до первого пробела в каждом предложении, \n",
    "            #заменяем первую череду пробелов в предложении\n",
    "            #на это кол-во символов\n",
    "            if sents[sent_id].find('---')>=0 or re.search('\\d\\.\\d', sents[sent_id]) is None and sents[sent_id].find('f1-score')<0:\n",
    "                sents[sent_id] = '\\n' + sents[sent_id]\n",
    "                continue\n",
    "            if sents[sent_id].find('MainThread')>=0:\n",
    "                break\n",
    "            sents[sent_id] = re.sub('\\s+', count_symbols_before_first_whitespace, sents[sent_id], count=1)\n",
    "            sents[sent_id] = re.sub('(?<=\\d)\\s+', ' '*11, sents[sent_id], count=1)\n",
    "            #след. строчка для того, чтобы цифры 0.00 норм обрабатывались\n",
    "            sents[sent_id] = re.sub('(?<=\\W0\\.00)\\s+', ' '*12, sents[sent_id], count=1)\n",
    "            delete_n_first_whitespaces.n=2\n",
    "            sents[sent_id] = re.sub('\\s+\\d+\\.\\d+\\s+\\d+$', delete_n_first_whitespaces, sents[sent_id])\n",
    "            sents[sent_id] = re.sub('\\s+(\\d+)$', ' '*9+r'\\1', sents[sent_id])\n",
    "            #след. строчка для того, чтобы цифры 0.00 норм обрабатывались\n",
    "            sents[sent_id] = re.sub('(?<=\\W0\\.00)\\s+(\\d+)$', ' '*10+r'\\1', sents[sent_id])\n",
    "\n",
    "        max_margin = max(symbols_before_first_whitespace)+4\n",
    "        spert_reports[rep_id] = '\\n'.join(sents)\n",
    "        spert_reports[rep_id] = re.sub('<\\d+>', make_valid_margin, spert_reports[rep_id])\n",
    "\n",
    "    #дальше смотрим только NER результат\n",
    "    NER_test_res = re.findall('{[^{}]+}', txt)[-1]\n",
    "    #дампим вывод\n",
    "    out_txt = \"====NER result====\\n\\n\"\n",
    "    out_txt +=  NER_test_res\n",
    "    out_txt += '\\n\\n====RE results====\\n\\n'\n",
    "    ##   Если всего 2 сперт репорта, то нужно оставить эти строчки\n",
    "    out_txt = out_txt + '====Gold NER====\\n' + spert_reports[0] + '====Pred NER====\\n' + spert_reports[1]\n",
    "    out_file = dirname(realpath(inp_file)) + '/' + 'pretty_' + file\n",
    "    ##\n",
    "    \n",
    "    with open(out_file, 'w') as f:\n",
    "        f.write(out_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#если несколько фолдов\n",
    "#сделаем таблички, а также агрегируем точности\n",
    "import pandas as pd\n",
    "\n",
    "NER_res_dfs = []\n",
    "RE_gold_NER_dfs = []\n",
    "RE_pred_NER_dfs = []\n",
    "\n",
    "for file in os.listdir(dirname(realpath(inp_file))):\n",
    "    if file.find('pretty_')<0:\n",
    "        continue\n",
    "    with open(dirname(realpath(inp_file)) + '/' + file, 'r') as f:\n",
    "        txt = f.read()\n",
    "    file = file.replace('pretty_', '')\n",
    "    fold_num = file.split('_')[0]\n",
    "    file = re.sub('\\d+_', '', file)\n",
    "    all_eval_parts = re.split('====[^=]+====', txt)\n",
    "    NER_res, RE_gold_NER, RE_pred_NER = [part for part in all_eval_parts if re.search('[^\\n]', part)]\n",
    "    #NER\n",
    "    NER_res = re.sub(\"\\n|\\s|{|}|'\", '', NER_res)\n",
    "    ents_d = {ent_res.split(':')[0]:float(ent_res.split(':')[1]) for ent_res in NER_res.split(',')}\n",
    "    NER_df = pd.DataFrame({'f1_conll': ents_d.values()}, index=ents_d.keys())\n",
    "    NER_res_dfs.append(NER_df)\n",
    "    out_file = realpath(dirname(realpath(inp_file))) + '/' + fold_num + '_' + 'NER' + '_' + file.replace('.out', '.csv')\n",
    "    NER_df.to_csv(out_file)\n",
    "    #RE gold NER\n",
    "    RE_gold_NER = RE_gold_NER.split('A relation is considered correct if the relation type and the two related entities are predicted correctly (in span and entity type)')[-1]\n",
    "    RE_gold_NER = re.sub('Parse.+\\n', '', RE_gold_NER)\n",
    "    RE_gold_NER = [row for row in RE_gold_NER.split('\\n') if row]\n",
    "    col_names = re.split('\\s+', RE_gold_NER[0])[1:]\n",
    "    rel_df =  pd.DataFrame(columns=col_names)\n",
    "    for row in RE_gold_NER[1:]:\n",
    "        row = re.split('\\s+', row)\n",
    "        rel_df = rel_df.append(pd.Series(map(float, row[1:]), index=col_names, name=row[0].replace('none_', '')))\n",
    "    rel_df['support'] = rel_df['support'].apply(int)\n",
    "    RE_gold_NER_dfs.append(rel_df)\n",
    "    out_file = realpath(dirname(realpath(inp_file))) + '/' + fold_num + '_' + 'RE_gold_NER' + '_' + file.replace('.out', '.csv')\n",
    "    rel_df.to_csv(out_file)\n",
    "    #RE pred NER\n",
    "    RE_pred_NER = RE_pred_NER.split('A relation is considered correct if the relation type and the two related entities are predicted correctly (in span and entity type)')[-1]\n",
    "    RE_pred_NER = re.sub('Parse.+\\n', '', RE_pred_NER)\n",
    "    RE_pred_NER = [row for row in RE_pred_NER.split('\\n') if row]\n",
    "    col_names = re.split('\\s+', RE_pred_NER[0])[1:]\n",
    "    rel_df =  pd.DataFrame(columns=col_names)\n",
    "    for row in RE_pred_NER[1:]:\n",
    "        row = re.split('\\s+', row)\n",
    "        rel_df = rel_df.append(pd.Series(map(float, row[1:]), index=col_names, name=row[0].replace('none_', '')))\n",
    "    rel_df['support'] = rel_df['support'].apply(int)\n",
    "    RE_pred_NER_dfs.append(rel_df)\n",
    "    out_file = realpath(dirname(realpath(inp_file))) + '/' + fold_num + '_' + 'RE_pred_NER' + '_' + file.replace('.out', '.csv')\n",
    "    rel_df.to_csv(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создаем одну общую таблицу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_NER_res_dfs = sum(NER_res_dfs)/len(NER_res_dfs)\n",
    "avg_NER_res_dfs['std']=float(0)\n",
    "for rel_type in avg_NER_res_dfs.index:\n",
    "    #print(np.std(list(map(lambda x: x['f1_conll'][rel_type], NER_res_dfs))))\n",
    "    avg_NER_res_dfs['f1_conll'][rel_type] = np.round(avg_NER_res_dfs['f1_conll'][rel_type], 2)\n",
    "    avg_NER_res_dfs['std'][rel_type] = np.round(np.std(list(map(lambda x: x['f1_conll'][rel_type], NER_res_dfs))), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_conll</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADR</th>\n",
       "      <td>56.72</td>\n",
       "      <td>2.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Disease_DisTypeDiseasename</th>\n",
       "      <td>87.23</td>\n",
       "      <td>2.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Disease_DisTypeIndication</th>\n",
       "      <td>63.74</td>\n",
       "      <td>3.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medication_MedTypeDrugname</th>\n",
       "      <td>96.43</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medication_MedTypeSourceInfodrug</th>\n",
       "      <td>63.01</td>\n",
       "      <td>5.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  f1_conll   std\n",
       "ADR                                  56.72  2.96\n",
       "Disease_DisTypeDiseasename           87.23  2.72\n",
       "Disease_DisTypeIndication            63.74  3.81\n",
       "Medication_MedTypeDrugname           96.43  0.85\n",
       "Medication_MedTypeSourceInfodrug     63.01  5.39"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_NER_res_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_NER_res_dfs.to_csv(realpath(dirname(realpath(inp_file)))+'/'+'avg_NER_res.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RE tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_f1_gold_NER_dfs = list(map(lambda x: pd.DataFrame(x['f1-score']),RE_gold_NER_dfs))\n",
    "RE_f1_pred_NER_dfs = list(map(lambda x: pd.DataFrame(x['f1-score']),RE_pred_NER_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "avg_RE_gold_NER_df = sum(RE_f1_gold_NER_dfs)/len(RE_f1_gold_NER_dfs)\n",
    "avg_RE_gold_NER_df['std']=float(0)\n",
    "for rel_type in avg_RE_gold_NER_df.index:\n",
    "    print(np.std(list(map(lambda x: x['f1-score'][rel_type], RE_gold_NER_dfs))))\n",
    "    avg_RE_gold_NER_df['std'][rel_type] = np.round(np.std(list(map(lambda x: x['f1-score'][rel_type], RE_gold_NER_dfs))), 3)\n",
    "\n",
    "#avg_RE_pred_NER_df = sum(RE_f1_pred_NER_dfs)/len(RE_f1_pred_NER_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_RE_gold_NER_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '''import numpy as np\n",
    "\n",
    "avg_RE_gold_NER_df = sum(RE_f1_gold_NER_dfs)/len(RE_f1_gold_NER_dfs)\n",
    "avg_RE_gold_NER_df['std']=0\n",
    "for rel_type in avg_RE_gold_NER_df.index:\n",
    "    print(np.std(list(map(lambda x: x['f1-score'][rel_type], RE_gold_NER_dfs))))\n",
    "    avg_RE_gold_NER_df['std'][rel_type] = float(np.std(list(map(lambda x: x['f1-score'][rel_type], RE_gold_NER_dfs)))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "avg_RE_pred_NER_df = sum(RE_f1_pred_NER_dfs)/len(RE_f1_pred_NER_dfs)\n",
    "avg_RE_pred_NER_df['f1-score'] = np.round(avg_RE_pred_NER_df['f1-score'], 2)\n",
    "avg_RE_pred_NER_df['std']=float(0)\n",
    "for rel_type in avg_RE_pred_NER_df.index:\n",
    "    avg_RE_pred_NER_df.at[rel_type, 'std'] = np.round(np.std(list(map(lambda x: x['f1-score'][rel_type], RE_pred_NER_dfs))), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RE_f1_pred_NER_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1-score</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADR_Drugname_0</th>\n",
       "      <td>48.07</td>\n",
       "      <td>3.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADR_Drugname_1</th>\n",
       "      <td>52.74</td>\n",
       "      <td>4.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diseasename_Indication_0</th>\n",
       "      <td>10.63</td>\n",
       "      <td>7.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diseasename_Indication_1</th>\n",
       "      <td>52.12</td>\n",
       "      <td>7.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drugname_Diseasename_0</th>\n",
       "      <td>61.86</td>\n",
       "      <td>3.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drugname_Diseasename_1</th>\n",
       "      <td>77.90</td>\n",
       "      <td>2.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drugname_SourceInfodrug_0</th>\n",
       "      <td>40.09</td>\n",
       "      <td>6.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drugname_SourceInfodrug_1</th>\n",
       "      <td>49.97</td>\n",
       "      <td>3.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro</th>\n",
       "      <td>49.17</td>\n",
       "      <td>2.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro</th>\n",
       "      <td>57.66</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           f1-score   std\n",
       "ADR_Drugname_0                48.07  3.53\n",
       "ADR_Drugname_1                52.74  4.44\n",
       "Diseasename_Indication_0      10.63  7.45\n",
       "Diseasename_Indication_1      52.12  7.72\n",
       "Drugname_Diseasename_0        61.86  3.52\n",
       "Drugname_Diseasename_1        77.90  2.09\n",
       "Drugname_SourceInfodrug_0     40.09  6.42\n",
       "Drugname_SourceInfodrug_1     49.97  3.65\n",
       "macro                         49.17  2.09\n",
       "micro                         57.66  2.00"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_RE_pred_NER_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_RE_pred_NER_df.to_csv(realpath(dirname(realpath(inp_file)))+'/'+'avg_RE_pred_NER_res.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>После 4 NER</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADR_Drugname_0</th>\n",
       "      <th>ADR_Drugname_1</th>\n",
       "      <th>Drugname_Diseasename_0</th>\n",
       "      <th>Drugname_Diseasename_1</th>\n",
       "      <th>Drugname_SourceInfodrug_0</th>\n",
       "      <th>Drugname_SourceInfodrug_1</th>\n",
       "      <th>Diseasename_Indication_0</th>\n",
       "      <th>Diseasename_Indication_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.07</td>\n",
       "      <td>52.74</td>\n",
       "      <td>61.86</td>\n",
       "      <td>77.9</td>\n",
       "      <td>40.09</td>\n",
       "      <td>49.97</td>\n",
       "      <td>10.63</td>\n",
       "      <td>52.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ADR_Drugname_0  ADR_Drugname_1  Drugname_Diseasename_0  \\\n",
       "0           48.07           52.74                   61.86   \n",
       "\n",
       "   Drugname_Diseasename_1  Drugname_SourceInfodrug_0  \\\n",
       "0                    77.9                      40.09   \n",
       "\n",
       "   Drugname_SourceInfodrug_1  Diseasename_Indication_0  \\\n",
       "0                      49.97                     10.63   \n",
       "\n",
       "   Diseasename_Indication_1  \n",
       "0                     52.12  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#flatten df\n",
    "sorted_rel_types = ['ADR_Drugname', 'Drugname_Diseasename', 'Drugname_SourceInfodrug', 'Diseasename_Indication']\n",
    "pos_neg_order = ['0', '1']\n",
    "\n",
    "sorted_rel_types = [[rel_type + '_' + rel_class for rel_class in pos_neg_order] for rel_type in sorted_rel_types]\n",
    "sorted_rel_types = sum(sorted_rel_types, [])\n",
    "flatten_RE_df = pd.DataFrame({k: [float(0)] for k in sorted_rel_types})\n",
    "for rel_type in sorted_rel_types:\n",
    "    flatten_RE_df[rel_type][0] = np.round(avg_RE_pred_NER_df['f1-score'][rel_type], 2)\n",
    "out_file = realpath(dirname(realpath(inp_file))) + '/avg_flattened_pred_NER_RE_df.csv'\n",
    "flatten_RE_df.to_csv(out_file, index=False)\n",
    "flatten_RE_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>До 4ых NER</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADR_Drugname_0</th>\n",
       "      <th>ADR_Drugname_1</th>\n",
       "      <th>Drugname_Diseasename_0</th>\n",
       "      <th>Drugname_Diseasename_1</th>\n",
       "      <th>Drugname_SourceInfodrug_0</th>\n",
       "      <th>Drugname_SourceInfodrug_1</th>\n",
       "      <th>Diseasename_Indication_0</th>\n",
       "      <th>Diseasename_Indication_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43.62</td>\n",
       "      <td>50.54</td>\n",
       "      <td>63.49</td>\n",
       "      <td>78.04</td>\n",
       "      <td>43.04</td>\n",
       "      <td>50.66</td>\n",
       "      <td>10.58</td>\n",
       "      <td>48.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ADR_Drugname_0  ADR_Drugname_1  Drugname_Diseasename_0  \\\n",
       "0           43.62           50.54                   63.49   \n",
       "\n",
       "   Drugname_Diseasename_1  Drugname_SourceInfodrug_0  \\\n",
       "0                   78.04                      43.04   \n",
       "\n",
       "   Drugname_SourceInfodrug_1  Diseasename_Indication_0  \\\n",
       "0                      50.66                     10.58   \n",
       "\n",
       "   Diseasename_Indication_1  \n",
       "0                     48.77  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#flatten df\n",
    "sorted_rel_types = ['ADR_Drugname', 'Drugname_Diseasename', 'Drugname_SourceInfodrug', 'Diseasename_Indication']\n",
    "pos_neg_order = ['0', '1']\n",
    "\n",
    "sorted_rel_types = [[rel_type + '_' + rel_class for rel_class in pos_neg_order] for rel_type in sorted_rel_types]\n",
    "sorted_rel_types = sum(sorted_rel_types, [])\n",
    "flatten_RE_df = pd.DataFrame({k: [float(0)] for k in sorted_rel_types})\n",
    "for rel_type in sorted_rel_types:\n",
    "    flatten_RE_df[rel_type][0] = np.round(avg_RE_pred_NER_df['f1-score'][rel_type], 2)\n",
    "out_file = realpath(dirname(realpath(inp_file))) + '/avg_flattened_pred_NER_RE_df.csv'\n",
    "flatten_RE_df.to_csv(out_file, index=False)\n",
    "flatten_RE_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1-score</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADR_Drugname_0</th>\n",
       "      <td>83.520</td>\n",
       "      <td>4.997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADR_Drugname_1</th>\n",
       "      <td>91.672</td>\n",
       "      <td>4.094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diseasename_Indication_0</th>\n",
       "      <td>33.140</td>\n",
       "      <td>4.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diseasename_Indication_1</th>\n",
       "      <td>87.124</td>\n",
       "      <td>0.681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drugname_Diseasename_0</th>\n",
       "      <td>76.070</td>\n",
       "      <td>1.467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drugname_Diseasename_1</th>\n",
       "      <td>88.372</td>\n",
       "      <td>1.281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drugname_SourceInfodrug_0</th>\n",
       "      <td>81.014</td>\n",
       "      <td>1.697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drugname_SourceInfodrug_1</th>\n",
       "      <td>91.946</td>\n",
       "      <td>0.307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro</th>\n",
       "      <td>79.106</td>\n",
       "      <td>1.463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro</th>\n",
       "      <td>85.080</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           f1-score    std\n",
       "ADR_Drugname_0               83.520  4.997\n",
       "ADR_Drugname_1               91.672  4.094\n",
       "Diseasename_Indication_0     33.140  4.411\n",
       "Diseasename_Indication_1     87.124  0.681\n",
       "Drugname_Diseasename_0       76.070  1.467\n",
       "Drugname_Diseasename_1       88.372  1.281\n",
       "Drugname_SourceInfodrug_0    81.014  1.697\n",
       "Drugname_SourceInfodrug_1    91.946  0.307\n",
       "macro                        79.106  1.463\n",
       "micro                        85.080  0.986"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_RE_gold_NER_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Drugname_Diseasename_0</th>\n",
       "      <td>80.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drugname_SourceInfodrug_0</th>\n",
       "      <td>81.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diseasename_Indication_1</th>\n",
       "      <td>87.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADR_Drugname_1</th>\n",
       "      <td>94.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diseasename_Indication_0</th>\n",
       "      <td>37.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drugname_Diseasename_1</th>\n",
       "      <td>90.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADR_Drugname_0</th>\n",
       "      <td>89.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drugname_SourceInfodrug_1</th>\n",
       "      <td>92.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro</th>\n",
       "      <td>87.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro</th>\n",
       "      <td>81.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           f1-score\n",
       "Drugname_Diseasename_0        80.20\n",
       "Drugname_SourceInfodrug_0     81.48\n",
       "Diseasename_Indication_1      87.89\n",
       "ADR_Drugname_1                94.71\n",
       "Diseasename_Indication_0      37.44\n",
       "Drugname_Diseasename_1        90.62\n",
       "ADR_Drugname_0                89.17\n",
       "Drugname_SourceInfodrug_1     92.12\n",
       "micro                         87.26\n",
       "macro                         81.70"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(RE_gold_NER_dfs[0]['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADR_Drugname_0</th>\n",
       "      <td>88.448</td>\n",
       "      <td>79.772</td>\n",
       "      <td>83.810</td>\n",
       "      <td>183.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADR_Drugname_1</th>\n",
       "      <td>90.388</td>\n",
       "      <td>94.710</td>\n",
       "      <td>92.480</td>\n",
       "      <td>382.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diseasename_Indication_0</th>\n",
       "      <td>58.738</td>\n",
       "      <td>26.850</td>\n",
       "      <td>36.020</td>\n",
       "      <td>140.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diseasename_Indication_1</th>\n",
       "      <td>82.396</td>\n",
       "      <td>94.184</td>\n",
       "      <td>87.822</td>\n",
       "      <td>517.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drugname_Diseasename_0</th>\n",
       "      <td>82.660</td>\n",
       "      <td>75.410</td>\n",
       "      <td>78.522</td>\n",
       "      <td>430.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drugname_Diseasename_1</th>\n",
       "      <td>88.254</td>\n",
       "      <td>91.828</td>\n",
       "      <td>89.942</td>\n",
       "      <td>855.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drugname_SourceInfodrug_0</th>\n",
       "      <td>84.746</td>\n",
       "      <td>80.790</td>\n",
       "      <td>82.644</td>\n",
       "      <td>246.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drugname_SourceInfodrug_1</th>\n",
       "      <td>91.426</td>\n",
       "      <td>93.260</td>\n",
       "      <td>92.320</td>\n",
       "      <td>540.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro</th>\n",
       "      <td>83.382</td>\n",
       "      <td>79.600</td>\n",
       "      <td>80.444</td>\n",
       "      <td>3296.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro</th>\n",
       "      <td>86.400</td>\n",
       "      <td>86.400</td>\n",
       "      <td>86.400</td>\n",
       "      <td>3296.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           precision  recall  f1-score  support\n",
       "ADR_Drugname_0                88.448  79.772    83.810    183.4\n",
       "ADR_Drugname_1                90.388  94.710    92.480    382.6\n",
       "Diseasename_Indication_0      58.738  26.850    36.020    140.2\n",
       "Diseasename_Indication_1      82.396  94.184    87.822    517.6\n",
       "Drugname_Diseasename_0        82.660  75.410    78.522    430.6\n",
       "Drugname_Diseasename_1        88.254  91.828    89.942    855.4\n",
       "Drugname_SourceInfodrug_0     84.746  80.790    82.644    246.4\n",
       "Drugname_SourceInfodrug_1     91.426  93.260    92.320    540.0\n",
       "macro                         83.382  79.600    80.444   3296.2\n",
       "micro                         86.400  86.400    86.400   3296.2"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(RE_gold_NER_dfs)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Drugname_Diseasename_1</th>\n",
       "      <td>83.51</td>\n",
       "      <td>95.88</td>\n",
       "      <td>89.27</td>\n",
       "      <td>729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADR_Drugname_0</th>\n",
       "      <td>87.86</td>\n",
       "      <td>73.43</td>\n",
       "      <td>80.00</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diseasename_Indication_1</th>\n",
       "      <td>81.70</td>\n",
       "      <td>92.12</td>\n",
       "      <td>86.60</td>\n",
       "      <td>533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drugname_Diseasename_0</th>\n",
       "      <td>88.10</td>\n",
       "      <td>61.67</td>\n",
       "      <td>72.55</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADR_Drugname_1</th>\n",
       "      <td>87.03</td>\n",
       "      <td>94.62</td>\n",
       "      <td>90.66</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drugname_SourceInfodrug_0</th>\n",
       "      <td>88.04</td>\n",
       "      <td>77.88</td>\n",
       "      <td>82.65</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drugname_SourceInfodrug_1</th>\n",
       "      <td>92.07</td>\n",
       "      <td>96.04</td>\n",
       "      <td>94.01</td>\n",
       "      <td>556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diseasename_Indication_0</th>\n",
       "      <td>46.15</td>\n",
       "      <td>24.66</td>\n",
       "      <td>32.14</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro</th>\n",
       "      <td>85.17</td>\n",
       "      <td>85.17</td>\n",
       "      <td>85.17</td>\n",
       "      <td>3129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro</th>\n",
       "      <td>81.81</td>\n",
       "      <td>77.04</td>\n",
       "      <td>78.49</td>\n",
       "      <td>3129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          precision recall f1-score support\n",
       "Drugname_Diseasename_1        83.51  95.88    89.27     729\n",
       "ADR_Drugname_0                87.86  73.43    80.00     207\n",
       "Diseasename_Indication_1      81.70  92.12    86.60     533\n",
       "Drugname_Diseasename_0        88.10  61.67    72.55     360\n",
       "ADR_Drugname_1                87.03  94.62    90.66     390\n",
       "Drugname_SourceInfodrug_0     88.04  77.88    82.65     208\n",
       "Drugname_SourceInfodrug_1     92.07  96.04    94.01     556\n",
       "Diseasename_Indication_0      46.15  24.66    32.14     146\n",
       "micro                         85.17  85.17    85.17    3129\n",
       "macro                         81.81  77.04    78.49    3129"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RE_gold_NER_dfs[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type                                precision       recall     f1-score      support\n",
      "ADR_Drugname_1                      92.18           96.39      94.24         526\n",
      "Diseasename_Indication_1            89.10           95.96      92.40         545\n",
      "none_Drugname_Diseasename_0         80.87           81.58      81.22         456\n",
      "Drugname_SourceInfodrug_1           93.45           91.99      92.72         512\n",
      "none_ADR_Drugname_0                 88.12           76.22      81.74         185\n",
      "none_Diseasename_Indication_0       55.10           30.00      38.85         90\n",
      "none_Drugname_SourceInfodrug_0      83.98           86.69      85.32         248\n",
      "Drugname_Diseasename_1              89.60           89.16      89.38         812\n",
      "micro                               88.32           88.32      88.32         3374\n",
      "macro                               84.05           81.00      81.98         3374\n",
      "Parse data from /s/ls4/users/romanrybka/pharm_er/true_RE/Joint/data/RDRS_multicontext/2/test.json\n",
      "Parse data from /s/ls4/users/romanrybka/pharm_er/true_RE/Joint/results/RDRS_multicontext/2//RE/re_res_pred_pred_ner.json\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(RE_gold_NER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type                                precision       recall     f1-score      support\n",
      "ADR_Drugname_1                      92.18           96.39      94.24         526\n",
      "Diseasename_Indication_1            89.10           95.96      92.40         545\n",
      "none_Drugname_Diseasename_0         80.87           81.58      81.22         456\n",
      "Drugname_SourceInfodrug_1           93.45           91.99      92.72         512\n",
      "none_ADR_Drugname_0                 88.12           76.22      81.74         185\n",
      "none_Diseasename_Indication_0       55.10           30.00      38.85         90\n",
      "none_Drugname_SourceInfodrug_0      83.98           86.69      85.32         248\n",
      "Drugname_Diseasename_1              89.60           89.16      89.38         812\n",
      "micro                               88.32           88.32      88.32         3374\n",
      "macro                               84.05           81.00      81.98         3374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s= '''type                                precision       recall     f1-score      support\n",
    "ADR_Drugname_1                      92.18           96.39      94.24         526\n",
    "Diseasename_Indication_1            89.10           95.96      92.40         545\n",
    "none_Drugname_Diseasename_0         80.87           81.58      81.22         456\n",
    "Drugname_SourceInfodrug_1           93.45           91.99      92.72         512\n",
    "none_ADR_Drugname_0                 88.12           76.22      81.74         185\n",
    "none_Diseasename_Indication_0       55.10           30.00      38.85         90\n",
    "none_Drugname_SourceInfodrug_0      83.98           86.69      85.32         248\n",
    "Drugname_Diseasename_1              89.60           89.16      89.38         812\n",
    "micro                               88.32           88.32      88.32         3374\n",
    "macro                               84.05           81.00      81.98         3374\n",
    "Parse data from /s/ls4/users/romanrybka/pharm_er/true_RE/Joint/data/RDRS_multicontext/2/test.json\n",
    "Parse data from /s/ls4/users/romanrybka/pharm_er/true_RE/Joint/results/RDRS_multicontext/2//RE/re_res_pred_pred_ner.json\n",
    "'''\n",
    "print(re.sub('Parse.+\\n', '', s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Entities (named entity recognition (NER)) ---\n",
      "\n",
      "An entity is considered correct if the entity type and span is predicted correctly\n",
      "type                                precision       recall     f1-score      support\n",
      "Disease:DisTypeNegatedADE           100.00           100.00     100.00         189\n",
      "Medication:MedTypeFrequency         100.00           100.00     100.00         33\n",
      "Medication:MedTypeDrugclass         100.00           100.00     100.00         214\n",
      "Medication:MedTypeDrugBrand         100.00           100.00     100.00         410\n",
      "Disease:DisTypeBNE-Pos              100.00           100.00     100.00         380\n",
      "Medication:MedTypeDrugname          100.00           100.00     100.00         734\n",
      "Medication:MedTypeDosage            100.00           100.00     100.00         53\n",
      "Medication:MedTypeMedMaker          100.00           100.00     100.00         106\n",
      "Disease:DisTypeADE-Neg              100.00           100.00     100.00         4\n",
      "ADR                                 100.00           100.00     100.00         169\n",
      "Disease:DisTypeIndication           100.00           100.00     100.00         352\n",
      "Disease:DisTypeWorse                100.00           100.00     100.00         15\n",
      "Medication:MedTypeSourceInfodrug    100.00           100.00     100.00         190\n",
      "Medication:MedTypeDuration          100.00           100.00     100.00         97\n",
      "Medication:MedTypeDrugform          100.00           100.00     100.00         388\n",
      "Disease:DisTypeDiseasename          100.00           100.00     100.00         318\n",
      "Medication:MedTypeRoute             100.00           100.00     100.00         255\n",
      "micro                               100.00           100.00     100.00         3907\n",
      "macro                               100.00           100.00     100.00         3907\n",
      "\n",
      "--- Relations ---\n",
      "\n",
      "Without named entity classification (NEC)\n",
      "\n",
      "A relation is considered correct if the relation type and the spans of the two related entities are predicted correctly (entity type is not considered)\n",
      "type                                precision       recall     f1-score      support\n",
      "none_Drugname_Diseasename_0         84.54           76.28      80.20         430\n",
      "none_Drugname_SourceInfodrug_0      86.57           76.95      81.48         243\n",
      "Diseasename_Indication_1            81.02           96.04      87.89         480\n",
      "ADR_Drugname_1                      92.33           97.22      94.71         396\n",
      "none_Diseasename_Indication_0       66.67           26.03      37.44         146\n",
      "Drugname_Diseasename_1              88.47           92.88      90.62         843\n",
      "none_ADR_Drugname_0                 94.15           84.69      89.17         209\n",
      "Drugname_SourceInfodrug_1           89.87           94.49      92.12         526\n",
      "micro                               87.26           87.26      87.26         3273\n",
      "macro                               85.45           80.57      81.70         3273\n",
      "\n",
      "With named entity classification (NEC)\n",
      "\n",
      "A relation is considered correct if the relation type and the two related entities are predicted correctly (in span and entity type)\n",
      "type                                precision       recall     f1-score      support\n",
      "none_Drugname_Diseasename_0         84.54           76.28      80.20         430\n",
      "none_Drugname_SourceInfodrug_0      86.57           76.95      81.48         243\n",
      "Diseasename_Indication_1            81.02           96.04      87.89         480\n",
      "ADR_Drugname_1                      92.33           97.22      94.71         396\n",
      "none_Diseasename_Indication_0       66.67           26.03      37.44         146\n",
      "Drugname_Diseasename_1              88.47           92.88      90.62         843\n",
      "none_ADR_Drugname_0                 94.15           84.69      89.17         209\n",
      "Drugname_SourceInfodrug_1           89.87           94.49      92.12         526\n",
      "micro                               87.26           87.26      87.26         3273\n",
      "macro                               85.45           80.57      81.70         3273\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(RE_gold_NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "type                                precision       recall     f1-score      support\n",
      "none_Drugname_Diseasename_0         84.54           76.28      80.20         430\n",
      "none_Drugname_SourceInfodrug_0      86.57           76.95      81.48         243\n",
      "Diseasename_Indication_1            81.02           96.04      87.89         480\n",
      "ADR_Drugname_1                      92.33           97.22      94.71         396\n",
      "none_Diseasename_Indication_0       66.67           26.03      37.44         146\n",
      "Drugname_Diseasename_1              88.47           92.88      90.62         843\n",
      "none_ADR_Drugname_0                 94.15           84.69      89.17         209\n",
      "Drugname_SourceInfodrug_1           89.87           94.49      92.12         526\n",
      "micro                               87.26           87.26      87.26         3273\n",
      "macro                               85.45           80.57      81.70         3273\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(RE_gold_NER.split('A relation is considered correct if the relation type and the two related entities are predicted correctly (in span and entity type)')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADR': 56.35,\n",
       " 'Disease_DisTypeADE-Neg': 0.0,\n",
       " 'Disease_DisTypeBNE-Pos': 43.23,\n",
       " 'Disease_DisTypeDiseasename': 85.54,\n",
       " 'Disease_DisTypeIndication': 65.5,\n",
       " 'Disease_DisTypeNegatedADE': 51.61,\n",
       " 'Disease_DisTypeWorse': 10.0,\n",
       " 'Medication_MedFromDomestic': 77.96,\n",
       " 'Medication_MedFromForeign': 80.6,\n",
       " 'Medication_MedMakerDomestic': 82.93,\n",
       " 'Medication_MedMakerForeign': 71.19,\n",
       " 'Medication_MedTypeDosage': 61.94,\n",
       " 'Medication_MedTypeDrugBrand': 84.49,\n",
       " 'Medication_MedTypeDrugclass': 93.42,\n",
       " 'Medication_MedTypeDrugform': 92.79,\n",
       " 'Medication_MedTypeDrugname': 95.49,\n",
       " 'Medication_MedTypeDuration': 61.9,\n",
       " 'Medication_MedTypeFrequency': 59.26,\n",
       " 'Medication_MedTypeMedMaker': 89.83,\n",
       " 'Medication_MedTypeRoute': 65.33,\n",
       " 'Medication_MedTypeSourceInfodrug': 63.94}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '''{'ADR': 56.35,\n",
    " 'Disease_DisTypeADE-Neg': 0.0,\n",
    " 'Disease_DisTypeBNE-Pos': 43.23,\n",
    " 'Disease_DisTypeDiseasename': 85.54,\n",
    " 'Disease_DisTypeIndication': 65.5,\n",
    " 'Disease_DisTypeNegatedADE': 51.61,\n",
    " 'Disease_DisTypeWorse': 10.0,\n",
    " 'Medication_MedFromDomestic': 77.96,\n",
    " 'Medication_MedFromForeign': 80.6,\n",
    " 'Medication_MedMakerDomestic': 82.93,\n",
    " 'Medication_MedMakerForeign': 71.19,\n",
    " 'Medication_MedTypeDosage': 61.94,\n",
    " 'Medication_MedTypeDrugBrand': 84.49,\n",
    " 'Medication_MedTypeDrugclass': 93.42,\n",
    " 'Medication_MedTypeDrugform': 92.79,\n",
    " 'Medication_MedTypeDrugname': 95.49,\n",
    " 'Medication_MedTypeDuration': 61.9,\n",
    " 'Medication_MedTypeFrequency': 59.26,\n",
    " 'Medication_MedTypeMedMaker': 89.83,\n",
    " 'Medication_MedTypeRoute': 65.33,\n",
    " 'Medication_MedTypeSourceInfodrug': 63.94}'''\n",
    "\n",
    "s = re.sub(\"\\n|\\s|{|}|'\", '', s)\n",
    "ents_d = {ent_res.split(':')[0]:float(ent_res.split(':')[1]) for ent_res in s.split(',')}\n",
    "ents_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_conll</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADR</th>\n",
       "      <td>56.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Disease_DisTypeADE-Neg</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Disease_DisTypeBNE-Pos</th>\n",
       "      <td>43.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Disease_DisTypeDiseasename</th>\n",
       "      <td>85.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Disease_DisTypeIndication</th>\n",
       "      <td>65.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Disease_DisTypeNegatedADE</th>\n",
       "      <td>51.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Disease_DisTypeWorse</th>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medication_MedFromDomestic</th>\n",
       "      <td>77.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medication_MedFromForeign</th>\n",
       "      <td>80.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medication_MedMakerDomestic</th>\n",
       "      <td>82.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medication_MedMakerForeign</th>\n",
       "      <td>71.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medication_MedTypeDosage</th>\n",
       "      <td>61.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medication_MedTypeDrugBrand</th>\n",
       "      <td>84.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medication_MedTypeDrugclass</th>\n",
       "      <td>93.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medication_MedTypeDrugform</th>\n",
       "      <td>92.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medication_MedTypeDrugname</th>\n",
       "      <td>95.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medication_MedTypeDuration</th>\n",
       "      <td>61.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medication_MedTypeFrequency</th>\n",
       "      <td>59.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medication_MedTypeMedMaker</th>\n",
       "      <td>89.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medication_MedTypeRoute</th>\n",
       "      <td>65.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medication_MedTypeSourceInfodrug</th>\n",
       "      <td>63.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  f1_conll\n",
       "ADR                                  56.35\n",
       "Disease_DisTypeADE-Neg                0.00\n",
       "Disease_DisTypeBNE-Pos               43.23\n",
       "Disease_DisTypeDiseasename           85.54\n",
       "Disease_DisTypeIndication            65.50\n",
       "Disease_DisTypeNegatedADE            51.61\n",
       "Disease_DisTypeWorse                 10.00\n",
       "Medication_MedFromDomestic           77.96\n",
       "Medication_MedFromForeign            80.60\n",
       "Medication_MedMakerDomestic          82.93\n",
       "Medication_MedMakerForeign           71.19\n",
       "Medication_MedTypeDosage             61.94\n",
       "Medication_MedTypeDrugBrand          84.49\n",
       "Medication_MedTypeDrugclass          93.42\n",
       "Medication_MedTypeDrugform           92.79\n",
       "Medication_MedTypeDrugname           95.49\n",
       "Medication_MedTypeDuration           61.90\n",
       "Medication_MedTypeFrequency          59.26\n",
       "Medication_MedTypeMedMaker           89.83\n",
       "Medication_MedTypeRoute              65.33\n",
       "Medication_MedTypeSourceInfodrug     63.94"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'f1_conll': ents_d.values()}, index=ents_d.keys())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(re.split('====[^=]+====', txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./results_joint/RDRS_all_folds_2382077.out'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    }
   ],
   "source": [
    "print(NER_test_res[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "#читаем файл, делим на сперт репорты\n",
    "with open(inp_file, 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "txt_parts = re.split('Evaluation\\n\\n|--------------------------------------------------', txt)\n",
    "txt_parts = [txt_part for txt_part in txt_parts if txt_part]\n",
    "spert_reports = [txt_part for txt_part in txt_parts if re.search('--- Entities .+\\s+---', txt_part)]\n",
    "\n",
    "spert_reports = [re.sub('\\n\\s+', '\\n', rep) for rep in spert_reports]\n",
    "\n",
    "\n",
    "#основной код\n",
    "def count_symbols_before_first_whitespace(matchobj):\n",
    "    num_symbols = matchobj.span()[0]\n",
    "    symbols_before_first_whitespace.append(num_symbols)\n",
    "    return '<'+str(num_symbols)+'>'\n",
    "\n",
    "def make_valid_margin(matchobj):\n",
    "    margin = max_margin - int(matchobj.group(0)[1:-1])\n",
    "    return ' '*margin\n",
    "\n",
    "def delete_n_first_whitespaces(matchobj):\n",
    "    columns = matchobj.group()\n",
    "    first_n_whitespaces = re.match('\\s+', columns).span()[1]\n",
    "    return ' '*(first_n_whitespaces-delete_n_first_whitespaces.n)+columns[first_n_whitespaces:]\n",
    "\n",
    "\n",
    "\n",
    "for rep_id in range(len(spert_reports)):\n",
    "    sents = re.split('\\n+', spert_reports[rep_id])\n",
    "    #sents = spert_reports[rep_id].split('\\n')\n",
    "    symbols_before_first_whitespace = []\n",
    "    for sent_id in range(1, len(sents)):\n",
    "        #считаем кол-во символов до первого пробела в каждом предложении, \n",
    "        #заменяем первую череду пробелов в предложении\n",
    "        #на это кол-во символов\n",
    "        if sents[sent_id].find('---')>=0 or re.search('\\d\\.\\d', sents[sent_id]) is None and sents[sent_id].find('f1-score')<0:\n",
    "            sents[sent_id] = '\\n' + sents[sent_id]\n",
    "            continue\n",
    "        if sents[sent_id].find('MainThread')>=0:\n",
    "            break\n",
    "        sents[sent_id] = re.sub('\\s+', count_symbols_before_first_whitespace, sents[sent_id], count=1)\n",
    "        sents[sent_id] = re.sub('(?<=\\d)\\s+', ' '*11, sents[sent_id], count=1)\n",
    "        #след. строчка для того, чтобы цифры 0.00 норм обрабатывались\n",
    "        sents[sent_id] = re.sub('(?<=\\W0\\.00)\\s+', ' '*12, sents[sent_id], count=1)\n",
    "        delete_n_first_whitespaces.n=2\n",
    "        sents[sent_id] = re.sub('\\s+\\d+\\.\\d+\\s+\\d+$', delete_n_first_whitespaces, sents[sent_id])\n",
    "        sents[sent_id] = re.sub('\\s+(\\d+)$', ' '*9+r'\\1', sents[sent_id])\n",
    "        #след. строчка для того, чтобы цифры 0.00 норм обрабатывались\n",
    "        sents[sent_id] = re.sub('(?<=\\W0\\.00)\\s+(\\d+)$', ' '*10+r'\\1', sents[sent_id])\n",
    "        \n",
    "    max_margin = max(symbols_before_first_whitespace)+4\n",
    "    spert_reports[rep_id] = '\\n'.join(sents)\n",
    "    spert_reports[rep_id] = re.sub('<\\d+>', make_valid_margin, spert_reports[rep_id])\n",
    "\n",
    "#дампим вывод\n",
    "out_file = './results/pretty_print_positive_relations.txt'\n",
    "\n",
    "\n",
    "txt = '============\\n'.join(spert_reports)\n",
    "with open(out_file, 'w') as f:\n",
    "    f.write(txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
